# Attention Management in Human Activity Systems for Guiding Engineering Teams

## Surfing Uncertainty

### Self-Organizing around Prediction Error
Prediction error provides an organismically computable quantity apt to drive neural self-organization in many ways and at many temporal scales. We have seen this principle in action many times in previous chapters, but it is worth pausing to admire the potent sweep of self-organization that results. At the heart of the process lies a probabilistic generative model that progressively alters so as better to predict the plays of sensory data that impinge upon a biological organism or artificial agent. This results in learning that can separate out interacting bodily and environmental causes operating at varying scales of space and time. Such approaches describe a potent mechanism for self-organized, grounded, structural learning. Learning is now grounded because distal causes are uncovered only as a means of predicting the play of sensory data (a play that also reflects the organism’s own actions and interventions upon the world). Such learning is structure-revealing, unearthing complex patterns of interdependencies among causes operating at different scales of space and time. All this provides a kind of palette of predictive routines that can be combined in novel ways to deal with new situations.
Such systems are self-organizing because they are not aiming at any specific input-output mapping. Instead, they must discover the patterns of cascading regularity that best accommodate their own (partially self-induced) flows of sensory information. This is liberating because it means that such systems can deliver ways of knowing that are not tied  to the performance of specific tasks (although the plays of sensory data to be accommodated are themselves constrained by the broad forms of human activity).
Such systems are also deeply context-sensitive. This is because systemic response in any area or at any level is now answerable to the full downwards (and lateral) cascade of context-fixing information. This non-linear dynamical picture increases still further in complexity because the flow of influence is itself reconfigurable, as changing precision estimations alter moment-by-moment patterns of effective connectivity.
By self-organizing around prediction error, these architectures thus deliver a multiscale grip upon the organism-salient features of the world—a grip whose signature is the capacity to engage that world in ongoing cycles in which perception and action work together to quash high-precision prediction error.

### Efficiency and ‘The Lord’s Prior’
An important point to notice is that such systems do not simply cease to change and alter just because sensory prediction error has been successfully minimized. For there is, as we saw in chapter 8 , another (less frequently highlighted) factor that can still drive change and learning. That factor is efficiency. Efficiency (see, e.g., Barlow, 1961 ; Olshausen & Field, 1996 ) is intuitively the opposite of redundancy and excess. A scheme or strategy is efficient if it uses only the minimal resources necessary to do the job. A generative model that is rich enough to maintain an organismic grip upon the regularities important for selecting behaviour, but that does so using minimal energetic or representational resources (for example, using few parameters) is efficient in this sense. By contrast, a system that uses a large number of parameters to accommodate or respond to the same data is not therby rendered a ‘more accurate’ modeller of its world. On the contrary, the result will often be ‘over-fitting’ the observed data, some of which turns out to be merely ‘noise’ or random fluctuations around the informative signal.
The Optical Acceleration Cancellation procedure described in chapter 8 is a nice example of a model that combines low complexity (few parameters) with high behavioural leverage. At the most general level, the drive towards efficiency is simply part and parcel of the overall imperative to minimize the sum of sensory prediction errors. This involves finding the most parsimonious model that successfully engages the sensory flow. For the deep functional role of the prediction error  signal, or so I have argued, is not to recruit new and better hypotheses about the world, so much as to leverage sensory information so as to guide fluent engagements with those aspects of the world relevant to our current needs and projects.1
All this is nicely dramatized in Feldman’s (2013 , p. 15) discussion of the ‘Lord’s Prior’ where this rather mischievously names the misleading idea that ‘the optimal Bayesian observer is correctly tuned when its priors match those objectively in force in the environment’. The deep problems with such a notion emerge as soon as we reflect that active agents are not, at root, simply trying to model the data so much as to come up with recipes for acting appropriately in the world. This will mean separating agent-salient signal from noise, selectively ignoring much of what the sensory signal makes available. Moreover, ‘one would be unwise to fit one’s prior too closely to any finite set of observations about how the world behaves, because inevitably the observations are a mixture of reliable and ephemeral factors’ (Feldman, 2013 , p. 25).
There is no guarantee that online predictive learning will correctly separate out signal from noise in the efficient way that that this requires. But all is not lost. For efficiency can be increased (complexity reduced) even in the absence of ongoing data-driven learning. One way to do this is to ‘prune’ synaptic connectivity (perhaps, as speculated in chapter 3 , during sleep) by removing connections that are weak or redundant. The ‘skeletonization’ algorithm2 in connectionism (see Mozer & Smolensky, 1990 , and discussion in Clark, 1993 ) and the aptly named wake-sleep algorithm (Hinton et al., 1995 ) are early examples of such procedures, each aiming to deliver robust performance while systematically reducing representational excess. The major benefit of such pruning is improved generalization—an improved ability to use what you know in a wide range of superficially distinct (but fundamentally similar) cases. Synaptic pruning provides a plausible mechanism for improving efficiency and reducing model complexity—an effect that may most frequently occur when exteroceptive sensory systems are dampened or shut down as occurs during sleep (see, e.g., Gilestro, Tononi, & Cirelli, 2009; Tononi & Cirelli, 2006 ).

### Chaos and Spontaneous Cortical Activity
Synaptic pruning provides an endogenous means of improving our grip upon the world. It enables us to improve the grip of our models and strategiesby eliminating spurious information and associations, and thus avoiding—or at least repairing—the kind of ‘overfitting’ that  occurs when a system uses valuable resources to track accidental or unimportant features of the training data. Synaptic pruning of this kind is best seen as a mechanism for improving the models that we already, in some rough sense, command. But we routinely do much more than that. For we are capable of a kind of deliberate imaginative exploration of our own mental space. The rudiments of this capacity come for free (as we saw in chapter 3 ) with the use of hierarchical neural prediction as a means of driving learning, perception, and action. Creatures deploying that kind of strategy turned out to be natural imaginars, able to drive their own sensorimotor systems ‘from the top down’. Such creatures can benefit from the use of mental simulations that automatically respect the interlocking constraints implied by the generative model. Such simulations provide a means of getting the most out of the generative model that we already command, while synaptic pruning helps improve that model from within.
But all this can still sound somewhat conservative, as if we are doomed (until new experiences are constructed or intervene) to stay, broadly speaking, within the limits of our achieved world view. To glimpse the possibility of more radical forms of endogenous cognitive exploration, recall the account of spontaneous cortical activity briefly sketched in 6.6. According to that account (see Berkes et al., 2011 ; see also Sporns, 2010 chapter 8), such spontaneous activity is not ‘mere neural noise’. Instead, it reflects a creature’s overall model of the world. Evoked activity (the activity resulting from a specific external stimulus) then reflects that model as it is applied to a specific sensory input.
What this and other work (see Sadaghiani et al., 2010 ) suggests is that spontaneous cortical activity is an expression (a kind of gross signature) of the specific generative model underlying perception and action. According to such an account, ‘ongoing activity patterns reflect a historically informed internal model of causal dynamics in the world (that serves to generate predictions of future sensory input)’ (Sadaghiani et al., 2010 , p. 10). Combining this picture of spontaneous cortical activity with the suggestions (8.11) regarding self-organizing instability opens up an intriguing possibility for more radical explorations of cognitive space.
Suppose that our acquired world model is implemented by a dynamical regime that is never quite stable, most likely due (see, e.g., Van Leeuwen, 2008) to various chaos-style effects. Under such conditions, the model itself (where this is nothing but the constellations of structured neural activity ready to guide perception and action) is constantly ‘flittering’, exploring the edges of its own territory. Variations in such activity would determine subtly different responses to the  encountered sensory stimuli. Even in the absence of compelling sensory inputs, that activity will not stop. Instead, it will continue to occur yielding ongoing forms of stimulus-detached exploration around the edges of the acquired model—explorations that, we may speculate, might suddenly result in a new or more imaginative (and often more parsimonious, see 9.3) solution to a problem or puzzle that has been occupying our attention. In addition, work by Coste et al. (2011) suggests that some spontaneous cortical activity is related to fluctuations in precision optimizations.3 Perhaps such fluctuations allow us to explore the edges of our own ‘meta-model’—our own estimates of context-relative reliability.
Might all this be at least part of the solution to deep and abiding puzzles concerning the origins of new ideas and creative problem-solving? Sadaghiani et al. (2010) link their account to some recent work in machine learning and robotics (Namikawa & Tani, 2010 ; Tsuda, 2001 ) in which such mental ‘wanderings’ are indeed the primary source of new forms of behaviour. Such wanderings might themselves be mandated by implicit hyperpriors that depict the world itself as changing and unstable, hence as no suitable place for systems that would rest on their cognitive laurels. Instead, we would be driven continuously to explore the edges of our own knowledge spaces, subtly altering our predictions and expectations (including our precision expectations) from moment to moment even in the absence of new information and experience.
In an interesting extension of these themes, Namikawa et al. (2011) explored the relationship between complex hierarchical structure and self-organizing instabilities (deterministic chaos) using neuro-robotic simulations. In this work, a generative model with multi-timescale dynamics enables a set of motor behaviours. In these simulations (just as in the PP models to which they are formally closely related):

Action per se, was a result of movements that conformed to the proprioceptive predictions of … joint angles [and] perception and action were both trying to minimize prediction errors throughout the hierarchy, where movement minimized the prediction errors at the level of proprioceptive sensations. (Namikawa et al., 2011 , p. 4)
In the simulations, deterministic chaos affecting slower timescale (higher level) network dynamics was found to enable new spontaneous transitions among primitive actions (the basic repertoire of the agent). This organization was shown to be both emergent and functionally crucial. It was emergent insofar as the concentration of chaotic dynamics in the higher level network occurred naturally as long as the  time constant of the higher level was significantly larger than that of the other regions (for the numerical details, see Namikawa et al., 2011 , p. 3). And this partial segregation of the chaotic dynamics was functionally crucial, since by restricting the impact of self-organized chaos to the higher level (slower timescale) networks, the robots were able to explore the useful, well-constrained space of possible action sequences without simultaneously undermining the stable, reusable elements of the action repertoire itself. They were thus able to generate new, spontaneous action transitions without disturbing the faster timescale dynamics (in lower level networks) that rendered their actions robust and reliably reproducible ‘on demand’.
Only the networks whose timescale dynamics were sufficiently spread out in this way proved able to display ‘both itinerant behaviors with accompanying spontaneous transitions of behavior primitives and intentional fixed behaviors (repeatedly executable) [using] the same dynamic mechanism’ (Namikawa et al., 2011 , p. 3, italics added). By contrast, if the timescale dynamics of the higher level network were reduced (becoming faster, hence closer to those of the lower level networks), robot behaviour became unstable and unreliable, leading the authors to conclude that ‘hierarchical timescale differences … are essential for achieving the two functions of freely combining actions in a compositional manner and generating them stably in a physical environment’ (Namikawa et al., 2011 , p. 9). Such results, preliminary though they are, begin to suggest something of the deep functional role of multiple timescale dynamics—dynamics that occur naturally as a result of hierarchical predictive processing and that are plausibly realized by the spread of labour between neural structures with varying temporal response characteristics.4

### Designer Environments and Cultural Practices
There is nothing specifically human, however, about any of the mechanisms for improving and exploring mental space just scouted. Prediction-driven learning, imagination, limited forms of simulation, and the canny exploitation of multi-timescale dynamics are all plausibly displayed, albeit to varying degrees, by other mammals. The most basic elements of the predictive processing story, as Roepstorff (2013 , p. 224) correctly notes, may thus be found in many types of organism and model system. The neocortex (the layered structure housing cortical columns that provides the most compelling neural implementation for predictive processing machinery) displays some dramatic variations  in size but is common to all mammals. Core features of the PP model may also be supported in other species using other structures (e.g., the so-called ‘mushroom bodies’ found in insect brains are conjectured to provide a means of implementing forward models used for prediction, see Li & Strausfeld, 1999 , and discussion in Webb, 2004 ).
What, then, makes us (superficially at least) so very different? What is it that allows us—unlike dogs, chimps, or dolphins—to latch on to distal causes that include not just food, mates, and relative social rankings but also neurons, predictive processing, Higgs bosons, and black holes? One possibility (Conway & Christiansen, 2001 ) is that adaptations of the human neural apparatus have somehow conspired to create, in us, an even more complex and context-flexible hierarchical learning system than is found in other animals. Insofar as the PP framework allows for rampant context-dependent influence within the distributed system, the same basic operating principles might (given a few new opportunities of routing and influence) result in the emergence of qualitatively novel forms of behaviour and control. Such changes might explain why human agents display what Spivey (2007 , p. 169) nicely describes as an ‘exceptional sensitivity to hierarchical structure in any time-dependent signal’.
Another (possibly linked and certainly highly complementary) possibility involves a potent complex of features of human life, especially our abilities of temporally co-coordinated social interaction (see Roepstorff, 2013 ) and our abilities to construct artefacts, and designer environments. Some of these ingredients have emerged in other species too. But in the human case the whole mosaic comes together under the influence of flexible structured symbolic language (this was the target of the Conway and Christiansen treatment mentioned above) and an almost obsessive drive (Tomasello et al., 2005 ) to engage in shared cultural practices. We are thus enabled repeatedly to redeploy our core cognitive skills in the transformative context of exposure to what Roepstorff et al. (2010) call ‘patterned sociocultural practices’. These include the use of symbolic inscriptions (encountered as ‘material symbols’, see Clark, 2006 ) embedded in complex practices and social routines (Hutchins, 1995 , 2014 ). Such environments and practices include those of mathematics, reading,5 writing, structured discussion, and schooling. The succession and tuning of such designer environments then constitutes the complex and ill-understood process that Sterelny (2003) describes as ‘incremental downstream epistemic engineering’.
What are the potential effects of such stacked and transmissible structures (designer environments and practices) upon prediction-driven learning in neural systems? Prediction-driven  learning routines make human minds permeable, at multiple spatial and temporal scales, to the statistical structure of the action-ready, organism-salient world, as reflected in the training signals. But those training signals are now delivered as part of a complex developmental web that gradually comes to include all the complex regularities embodied in the web of statistical relations among the symbols and other forms of sociocultural scaffolding in which we are immersed. We thus self-construct a kind of rolling ‘cognitive niche’ able to induce the acquisition of generative models whose reach and depth far exceeds their apparent base in simple forms of sensory contact with the world.
To see how this might work, recall that the way to construct a new idea or concept (assuming the resources of PP) is to encounter a new sensory pattern that results in highly weighted (organism-salient) prediction error. Highly weighted errors, if the system is unable to explain them away by recruiting some model that it already commands, result in increased plasticity and (if all goes well) the acquisition of new knowledge about the shape and nature of the distal causes responsible for the surprising sensory inputs. But we humans are also expert at deliberately manipulating our physical and social worlds so that they provide new and ever-more-challenging patterns that will drive new learning. A very simple example is the way that learning to perform mental arithmetic has been scaffolded, in some cultures, by the deliberate use of an abacus. Experience with the sensory patterns thus made available helps to install appreciation of many complex arithmetical operations and relations (for discussion, see Stigler et al., 1986 ). The specific example does not matter very much, but the general strategy does. We structure (and repeatedly restructure) our physical and social environments in ways that make available new knowledge and skills (for some lovely explorations, see Goldstone, Landy, & Brunel, 2011 ; Landy & Goldstone, 2005 ; and, for an information-theoretic twist, Salge, Glackin, & Polani, 2014 ). Prediction-hungry brains, exposed in the course of embodied action to novel patterns of sensory stimulation, may thus acquire forms of knowledge that were genuinely out-of-reach prior to such physical-manipulation-based retuning of the generative model.
Such retuning and enhancement is now served by a huge variety of symbol-mediated loops into material and social culture: loops that involve (see Clark, 2003 , 2008 ) notebooks, sketchpads, smartphones, and also (see Pickering & Garrod, 2007 ) written and spoken conversations with other agents.6 Such loops are effectively enabling new forms of re-entrant processing. They take a ‘first-order’ cognitive product (such as the visual experience of seeing a new purple skyscraper) clothe it in public symbols (turning it into the written or spoken sequence, “I saw  a new purple skyscraper today”) and launch it into the world so that it can re-enter our own cognitive system, and the cognitive systems of other agents, as a new kind of concrete perceptible—the percept of a written or spoken sentence (Clark 2006 , 2008 ). Those new perceptibles bear highly informative statistical relations to other such linguaform perceptibles. Once externalized, an idea or thought is thus able to participate in brand new webs of higher order and more abstract statistical correlation. The signature of such correlations is that words predict the occurrence of other words, tokens of mathematical symbols and operators predict the occurrence of other such tokens, and so on.
We glimpse the power of the complex internal statistical relationships enshrined in human languages in Landauer and colleagues’ fascinating work on ‘latent semantic analysis’ (LSA). This work reveals the vast amount of information now embodied in statistical (but deep, not first order) relations between words and the larger contexts (sentences and texts) in which they occur (see Landauer & Dumais, 1997 ; Landauer et al., 2007 ). For example, deep statistical relations among words were here shown to contain information that could help predict the grade-score of essays in specific subject areas. More generally (for LSA is a very specific technique with strict limitations) the rich symbolic world we humans immerse ourselves in is now demonstrably chock-full of information about meaning relations in itself . Those meaning relations are reflected in our patterns of use (hence in patterns of occurrence) and they can be identified and exploited regardless of the many more fundamental hooks that link words and symbols to practical actions and the (rest of our) sensory world. Some of those meaning relations, moreover, obtain in realms whose core constructs are now far, far removed from any simple sensory signatures, visible only in the internal relations proper to the arcane worlds of quantum theory, higher mathematics, philosophy, art, and politics (to name but a few).
Our best takes on the world are thus given material form and made available (in that new guise) as publically perceptible object—words, sentences, equations. An important side-effect of this is that our own thoughts and ideas now become available, to ourselves and others, as potential objects for deliberate processes of attention. This opens the door to a whole array of knowledge-improvement and knowledge-testing techniques, ranging from simple conversations in which we ask for reasons, to the complex practices of testing, dissemination, and peer-review characteristic of contemporary science. Courtesy of all that material public vehicling in spoken words, written text, diagrams, and pictures, our best predictive models of the  world (unlike those of other creatures) have thus become stable, reinspectable objects apt for public critique and systematic, multi-agent, multi-generational test and refinement. Our best models of the world are thus able to serve as the basis for cumulative, communally distributed reasoning, rather than just providing the means by which individual thoughts occur. The same potent predictive processing regimes, now targeting these brand new types of statistically pregnant ‘designer inputs’, are then enabled to discover and refine new generative models, latching onto (and at times actively creating) ever more abstract structure in the world. The upshot is that the human-built (material and sociocultural) environment becomes a potent source of new transmissible structure that trains, triggers, and repeatedly transforms the activity of the prediction-hungry biological brain.7
In sum, our human-built worlds are not merely the arenas in which we live, work, and play. They also structure the life-long statistical immersions that build and rebuild the generative models that inform each agent’s repertoire for perception, action, and reason. By constructing a succession of designer environments, such as the human-built worlds of education, structured play, art, and science, we repeatedly restructure our own minds. These designer environments have slowly become tailored to creatures like us, and they ‘know’ us as well as we know them. As a species, we refine them again and again, generation by generation. It is this iterative re-structuring, and not sheer processing power, memory, mobility, or even the learning algorithms themselves, that completes the human mental mosaic.

### White Lines
To further appreciate the power and scope of such culturally-mediated reshaping, recall the main moral of chapter 8 . The moral was that the predictive brain is not doomed to deploy high-cost, model-rich strategies moment-by-moment in a demanding and time-pressured world. Instead, action and environmental structuring can both be called upon to reduce complexity. In such cases, PP delivers and deploys low-cost strategies that make the most of body, world, and action. In the simple case of running to catch a fly ball, the problem to be solved was ‘posed’ by the very ball whose in-flight optical properties made available the low-cost solution itself. In such cases, we did not need to actively structure our world so as to make the low-cost strategy available, or cue its use. In other cases, however, the cultural snowball has enabled us to  structure our worlds in ways that both cue and help constitute low-cost routes to behavioural or cognitive success.
A maximally simple example is painting white lines along the edges of a winding cliff-top road. Such environmental alterations allow the driver to solve the complex problem of keeping the car on the road by (in part) predicting the ebb and flow of various simpler optical features and cues (see, e.g., Land, 2001 ). In such cases, we are building a better world to predict in, while simultaneously structuring the world to cue that strategy at the right time. In other words, we build worlds that cue simpler strategies that are only available because of the way we have altered the world in the first place. Other examples include the use of posted prices in supermarkets (Satz & Ferejohn, 1994 ), wearing the colours of our favourite football team, or displaying the distinctive clothing styles of our chosen subculture.
The full potential of the prediction error minimization model of how cortical processing most fundamentally operates may thus emerge only when that story is paired with an appreciation of what immersion in a huge variety of sociocultural designer environments can do (for some early steps in this direction, see Roepstorff et al., 2010 ). Such a combined approach would implement a version of ‘neuroconstructivism’ (Mareschal et al., 2007 ), a view according to which
The architecture of the brain … and the statistics of the environment, [are] not fixed. Rather, brain-connectivity is subject to a broad spectrum of input-, experience-, and activity-dependent processes which shape and structure its patterning and strengths. … These changes, in turn, result in altered interactions with the environment, exerting causal influences on what is experienced and sensed in the future. (Sporns, 2007 , p. 179)
Much of what is distinctive about human thought and reason may thus be best explained by the operation of what Hutchins (2014 , p. 35) describes as ‘cultural ecosystems operating at large spatial and temporal scales’. Within such ecosystems slowly evolved culturally transmitted practices sculpt the very worlds within which neural prediction error minimization occurs. Those cultural practices may themselves be usefully understood, Hutchins conjectures, as entropy (surprise) minimization devices operating at extended spatial and temporal scales. Action and perception then work together to reduce prediction error only against the more slowly evolving backdrop of a culturally distributed process that spawns a succession of practices and designer environments whose impact on the development (e.g., Smith & Gasser, 2005 ) and unfolding of human thought and reason can hardly be overestimated.
 There is a downside, of course. The downside is that these culturally mediated processes may also incur costs in the form of various kinds of path-dependence (Arthur, 1994 ) in which later solutions build on earlier ones. Sub-optimal path-based idiosyncrasies may then become frozen (perhaps like the much-discussed QWERTY keyboard or Betamax video format) into our material artefacts, institutions, notations, measuring tools, and cultural practices. But these costs are easy to bear. For it is those very same trajectory-sensitive cultural processes that deliver the vast cognitive profits that flow from the slow, multigenerational development of designer environments—environments that help human minds go where other animal minds do not.
9.7 Innovating for InnovationAdding further fuel to this sociocultural-technological fire, it may even be the case, as elegantly argued by Heyes (2012) , that many of our capacities for cultural learning are themselves cultural innovations, acquired by social interactions, rather than flowing directly from fundamental biological adaptations. The idea here is that:
the specialized features of cultural learning—the features that make cultural learning especially good at enabling the social transmission of information—are acquired in the course of development through social interaction. … They are products as well as producers of cultural evolution. (Heyes, 2012 , p. 2182)
Cultural learning, to borrow Heyes own analogy, would not merely be a producer of more and more ‘grist’ (transmissible facts about the world) but a source of ‘mills’—the ‘psychological processes that enable us to learn the grist from others’ (Heyes, 2012 , p. 2182).
The most obvious example is reading and writing, a matched pair of cultural practices that seem to have emerged far too recently to be a result of genetic adaptations. The practice of reading is known to cause widespread changes in human neural organization (Dehaene et al., 2010; Paulesu et al., 2000 ). The resulting new organization exploits what Anderson (2010) describes as a fundamental principle of ‘neural reuse’ in which pre-existing elements are recruited and repurposed. In this way:
learning to read takes old parts and remodels them into a new system. The old parts are computational processes and cortical  regions originally adapted, genetically and culturally, for object recognition and spoken language, but it is an ontogenetic, cultural process—literacy training—that makes them into a new system specialized for cultural learning. (Heyes, 2012 , p. 2182)
Reading is thus a nice example of a culturally inherited mill—a product of cultural evolution that feeds and fuels the process of cultural evolution itself. Other examples, Heyes argues, may include key mechanisms of social learning (learning by observing other agents in action) and imitation. If Heyes is right, then culture itself may be responsible for many of the sub-mechanisms that give the cultural snowball the means and momentum to deliver minds like ours.
9.8 Words as Tools for Manipulating PrecisionWords and phrases enjoy a double life. They function as communicative vehicles, but they also seem to play a role in the unfolding and development of our own thoughts and ideas. This latter role is sometimes referred to as the supra-communicative dimension of language (see Clark, 1998 ; Dennett, 1991 ; Jackendoff, 1996 ). This supra-communicative role can seem rather mysterious. What cognitive advantage could possibly accrue to an agent simply in virtue of expressing a thought (one that, you might insist, she already has) using language? The answer, presumably, is that we are wrong to depict the case in quite that way. Rather than merely expressing a thought we already have, such acts must somehow alter, impact, or transform the thinking itself. But just how might this work?
Consider, from the PP perspective, the likely effects of encountered, self-produced, or conversationally co-constructed words or phrases upon individual processing and problem-solving. In such cases we, either alone or as part of a collective, are creating ‘artificial input streams’ that may be peculiarly well-adapted to alter and nuance the flows of inner processing that help determine perception, experience, and action.
In a bare-bones exploration of such ideas, Lupyan and Ward (2013) conducted an experiment using a technique called Continuous Flash Suppression (CFS).8 In CFS an image continuously presented to one eye is suppressed when a changing stream of other images is presented to the other eye. This is another example of bi-stable perception, related to the binocular rivalry case that we explored way back in chapter 1 .9 Lupyan and Ward found that an object that is masked from  awareness by CFS can be unsuppressed (consciously detected) if the right word—the word ‘zebra’, if the suppressed object was a zebra—is heard before the trial begins. Hearing the right word increased the ‘hit rate’ for detecting the object and shortened reaction times too. The explanation, the authors suggest, is that ‘when information associated with verbal labels matches incoming (bottom-up) activity, language provides a top-down boost to perception, propelling an otherwise invisible image into awareness’ (Ward & Lupyan, 2013 , p. 14196). In this experiment the verbal enhancement was externally provided. But related effects, in which no external cueing is provided, have also been demonstrated for conscious recognition. Thus, Melloni et al. (2011 , and discussion in 3.6) showed that the onset time required to form a reportable conscious percept varies substantially (by around 100 ms) according to the presence or absence of apt expectations, even when those expectations emerge naturally as the subject performs a task. Putting these two effects together suggests that exposure to words functions to alter or nuance the active expectations that help construct our ongoing experience.
In some ways, this seems obvious enough. Words make a difference! But there is emerging evidence that the expectations induced by exposure to words and phrases are especially strong, focused, and targeted. Lupyan and Thompson-Schill (2012) found that hearing the word ‘dog’ is better than simply hearing a barking sound as a means of improving performance in a related discrimination task. There is also intriguing evidence (see Çukur et al., 2013, and discussion in Kim & Kasstner, 2013 ) that category-based attention (as when we are told covertly to attend to ‘vehicles’ or to ‘humans’ when watching a movie or video clip) temporarily alters the tuning of neuronal populations, shifting the category sensitivity of neurons or of neuronal ensembles in the direction of the attended content.
Such transient instruction-induced alterations of cortical representations could be cashed out by the suite of mechanisms that alter the precision-weighting of specific prediction error signals. This is suggestive. A potent feature of structured language is its ability to cheaply and very flexibly target highly specific aspects of our own understanding or of our understanding of another agent. One way in which this could work, within the context of a PP-style cognitive architecture, is thus by impacting our ongoing estimations of precision, hence the relative uncertainty assigned to different aspects of ongoing neural activity. Recent work by Yuval-Greenberg and Heeger (2013 , p. 9365) suggests that ‘CFS is based on modulating the gain of neural responses, akin to reducing target contrast’. The PP mechanism for modulating  such gain is, of course, the precision-weighting of prediction error. Language, it may be conjectured, provides a finely tuned means of artificially manipulating the precision (hence of temporarily modifying the impact) of prediction error at different levels of neural processing. Such transient, targeted, subtle manipulations of precision could selectively enhance or mute the influence of any aspect of our own or another agent’s world model. Self-produced (or mentally rehearsed) language would then emerge as a potent means of exploring and exploiting the full potential of our own acquired generative model, providing a kind of artificial second system for manipulating the precision-weighting of our own prediction errors—hence a ‘neat trick’ (Dennett, 1991 ) for artificially manipulating our estimations of our own uncertainty enabling us to make fully flexible use of what we know.
Words, we might say, are (for us language users) a metabolically cheap and flexible source of ‘artificial contexts’ (Lupyan & Clark, in press). Viewed from the PP perspective, the impact of strings of words upon neural processing is thus flexibly to modify both what top-down information is brought to bear, and how much influence it has at every level of processing (see Figure 9.1 ). Such a powerful tool for targeted  self-manipulation will provide a huge boost to intelligence, improving performance in ways that go far beyond those associated with linguistic performance alone.10 It is a tool, moreover, whose overall cognitive impact would be expected to vary in proportion to the subtlety and range of the linguistic repertoire of the agent.
This is a tempting picture indeed. But exactly how public linguaform encodings interact with the kinds of structured probabilistic knowledge representation posited by PP remains largely unknown. Such interactions lie at the heart of the processes of cultural construction described earlier and must constitute a crucial target for future research.

### Predicting with Others
In the social world, many of the tricks and ploys we have just sketched come together in a mutually supportive mix. Other agents are (recall chapter 5 ) often apt for prediction using the same generative model that gives rise to our own actions. But other agents also provide a unique form of ‘external scaffolding’, since their actions and responses can be exploited to reduce our own individual processing loads. Finally, other agents are themselves predictors, and this opens up an interesting space for mutually beneficial (or sometimes destructive—recall 2.9) processes of ‘continuous reciprocal prediction’.
A nice example, explored in some detail by Pickering and Garrod (2007 , 2013 ) is the co-construction of a conversation. In conversation, Pickering and Garrod suggest, each person uses their own language production system (hence the generative model underlying their own behaviour) to help predict the other’s utterances, while also using the output of the other as a kind of external scaffolding for their own ongoing productions. These predictions (just as PP would suggest) are probabilistic, and span multiple different levels from phonology to syntax and semantics. As conversation proceeds, multiple predictions are thus continuously co-computed with their associated probabilities (see also Cisek & Kalaska, 2011 ; Spivey, 2007 ). Each party to such a process is, in the typical case, in the business of matching, or attempting to match, their behaviour and expectations to those of the other. As conversation proceeds, words, grammar, intonation, gesture, and eye movements may all be overtly copied or covertly imitated (for a handy review of the linguistic and behavioural evidence, see Pickering & Garrod, 2004 ). Overt copying, in particular, helps support mutual prediction and mutual understanding since ‘if B overtly imitates A, then  A’s comprehension of B’s utterance is facilitated by A’s memory for A’s previous utterance’ (Pickering & Garrod, 2007 , p. 109). The upshot is that ‘prediction and imitation can jointly explain why conversation tends to be easy, even though it involves constant task-switching and the need to determine when to speak and what to say’ (p. 109).
Such piggybacking is not, of course, restricted to our conversational interactions. Instead, in one form or another it seems to characterize many forms of human joint action ranging from team sports to changing the bed linen with a partner (Sebanz & Knoblich, 2009 ). Individual agents may also actively constrain their own behaviour so as to make themselves more easily predictable by other agents. Thus, we might artificially stabilize our own public personas so as to encourage others to enter into economic or emotional arrangements with us (Ross, 2004 ). On an even grander scale, Colombo (in press) depicts social norms (the mostly unwritten ‘rules’ of daily social behaviour, such as leaving a tip in a restaurant) as devices whose role is to reduce mutual uncertainty by creating structures or schemas within which behaviour becomes more mutually predictable. Social norms, Colombo argues, are entropy-minimizing devices, represented as probability distributions, that serve to make social behaviour predictable. Expectations about our own behaviour are thus simultaneously descriptive and prescriptive in nature.
This dual nature is also evident (Hirsh et al., 2013) in the cognitive role of personal narratives: the stories we tell, to ourselves and to others, about the flow and meaning of our lives. Such narratives function as high-level elements in the models that structure our own self-predictions, and thus inform our own future actions and choices. But personal narratives are often co-constructed with others, and thus tend to feed the structures and expectations of society back in so that they become reflected in the models that an individual uses to make sense of her own acts and choices. Personal narratives may thus be considered as another species of communal uncertainty-reducing device.
Roepstorff and Frith (2004) note that many cases of human interaction involve a kind of top-level ‘script-sharing’ in which the highest-level processes that control one agent’s action may originate in the brain of another agent. The case they examine in detail is the construction of a sufficient understanding of an experimental situation to allow a subject to participate in a specific psychological experiment. In such cases, human agents can often achieve the (sometimes quite demanding) understanding needed to participate simply by means of a bout of verbal instruction in which high-level understandings are communicated directly from experimenter to subject. This is a case of what  Roepstorff and Frith engagingly dub ‘top-top control of action’, in which elements of the experimenter’s high-level understanding become positioned, courtesy of linguistic exchange, to control patterns of response in another agent. Such a situation may be contrasted with the long and arduous training process needed to install sufficient understanding in a monkey. The especially challenging example that Roepstorff and Frith describe involved performing a simplified version of the Wisconsin Card Sorting Task and required, before the monkeys were able to act as suitable subjects, a full year of operant conditioning (see Nakahara et al., 2002 ). Following this training, Nakahara et al. found anatomically similar brain activations in both monkeys and human subjects as they performed the task, suggesting that, as planned, the monkeys had indeed learnt the same ‘cognitive set’ (the same guiding recipe for action and choice) as the human subjects. But despite this end-point similarity, the process was, clearly, radically different, since:
whereas the human participant receives this script directly from the experimenter in a ‘top-top’ exchange, the monkey has to reconstruct this script solely via the concrete stimuli and rewards offered to it. It happens as the monkey, based on the previous understandings of the situation, reacts to the reward responses that the experimenter dispenses. (Roepstorff & Frith, 2004 , p. 193)

In the case of the monkey, script-synchronization required the experimenters’ top-level understanding to be recreated via a truly bottom-up process of learning, whereas in the case of the human subjects, this arduous route could be avoided by the judicious use of language and pre-existing shared understandings. Among human subjects already possessing significant shared understanding, language thus provides a kind of cheap, readily available ‘top-top’ route for the control of action.
Looping linguaform interactions can thus help create what Hasson et al. (2012 , p. 114) describe as systems of ‘brain-to-brain coupling’ in which ‘the perceptual system of one brain [is] coupled to the motor system of another’ in ways that enable the emergence of new forms of joint behaviour—for example, when one agent shouts commands to another while moving a grand piano up a flight of stairs.
Language also provides a means for whole groups of human agents to collectively negotiate complex representational spaces. In particular, it provides a means (see Clark, 1998 ) of taming ‘path-dependent’ learning. Path dependency, in its most familiar form, is the rationale for structured education and training. This is necessary because certain ideas can be understood only once others are in place. Such ‘cognitive  path dependency’ is nicely explained (see, e.g., Elman, 1993 ) by treating intellectual progress as involving something like a process of computational search in a large and complex space. Previous learning inclines the system to try out certain locations in the space and not others. When the prior learning is appropriate, the job of discovering some new regularity is made tractable: the prior learning acts as a filter on the space of options to be explored. The hierarchical nature of the prediction-based approaches we have been exploring makes them especially well-suited as inner mechanisms capable of supporting complex patterns of path-dependent learning in which later achievements build on earlier ones. At the same time, however, prior learning makes certain other regularities harder (at times impossible) to spot. Prior knowledge is thus always both constraining and enabling.
When confronting agents that exhibit path-dependent learning, the mundane observation that language allows ideas to be preserved and (in some sense) to migrate between individuals takes on a new force. For we can now appreciate how such migrations may allow the communal construction of extremely delicate and difficult intellectual trajectories and progressions. An idea which only Joe’s experience makes available, but which can flourish and realize its full potential only in the intellectual niche currently provided by the brain of Mary, can now realize its full potential by journeying between those agents. Different agents (and the same agent at different times) constitute different ‘filters’, and groups of such agents make available trajectories of learning and discovery that no single agent could comprehend. The variety of intellectual niches available within a linguistically linked community thus provides a stunning matrix of group-level multi-agent trajectories.
In sum, socially interacting agents benefit from nested and self-reinforcing cycles of ongoing mutual prediction. This kind of joint piggy-backing emerges naturally when groups of interacting, predictive agents construct a shared social world and may be a fundamental source of low-cost computational strategies for human interaction. Inter-agent exchanges thus create new paths through the space of possible understandings, allowing webs of communicating agents communally to explore intellectual trajectories that would rapidly defeat any individual agent.

### Enacting Our Worlds
The combined effects of action, cultural learning, reciprocal prediction, the canny use of language, and the many forms of socio-technological  scaffolding are transformative. It is the ill-understood alchemy between the predictive brain and this whole raft of mutually supportive tricks and ploys that makes us distinctively human. An immediate implication of our larger story is thus that there is a very real sense in which human agents help construct the very worlds they model and inhabit.
That process of construction corresponds rather closely to the mysterious-sounding notion of ‘enacting a world’, at least as that notion appears in Varela et al. (1991) .11
Varela et al. write that:
The overall concern of an enactive approach to perception is not to determine how some perceiver-independent world is to be recovered; it is, rather, to determine the common principles or lawful linkages between sensory and motor systems that explain how action can be perceptually-guided in a perceiver-dependent world. (Varela et al., 1991 , p. 173)
Such an approach to perception is prefigured, Varela et al. report, in the work of Merleau-Ponty (1945 /1962). There, Merleau-Ponty stresses the important degree to which perception itself is structured by human action. Thus, we often think of perception as simply the source of information that is then used for the guidance of action. But expand the temporal window a little and it becomes clear that we might equally well think of action as the selector of the perceptual stimulations themselves. In the words of Merleau-Ponty:
since all the stimulations which the organism receives have in turn been possible only by its preceding movements which have culminated in exposing the receptor organ to external influences, one could also say that behavior is the first cause of all the stimulations. (Merleau-Ponty, 1945 /1962, p. 13)
In a striking image, Merleau-Ponty then compares the active organism to a keyboard which moves itself around so as to offer different keys to the ‘in itself monotonous action of an external hammer’ (p. 13).12 The message that the world ‘types onto the perceiver’ is thus largely created (or so the image suggests) by the nature and action of the perceiver herself: the way she offers herself to the world. The upshot, according to Varela et al. (1991 , p. 174) is that ‘the organism and environment [are] bound together in reciprocal specification and selection’.
This kind of relation is described by Varela et al. as one of ‘structural coupling’ in which ‘the species brings forth and specifies its own domain of problems’ (p. 198) and in that sense ‘enacts’ or brings forth (p. 205) its own world. In discussing these matters, Varela et al. are  also concerned to stress that the relevant histories of structural coupling may select what they describe as ‘non-optimal’ features, traits, and behaviours: ones that involve ‘satisficing’ (see Simon, 1956 , and chapter 8) where that means settling for whatever ‘good enough’ solution or structure ‘has sufficient integrity to persist’ (Varela et al., 1991 , p. 196). PP has the resources to cash all these ‘enactivist’ cheques, depicting the organism and the organism-salient world as bound together in a process of mutual specification in which the simplest approximations apt to support a history of viable interaction are the ones that are learnt, selected, and maintained.13
The simplest way in which a predictive-processing enabled agent might be said to actively construct its world is by sampling. Action here serves perception by moving the body and sense organs around in ways that aim to ‘serve up’ predicted patterns of stimulation. In particular, they aim (chapter 2) to serve up predicted sequences of high-reliability, task-relevant information. This is a very clear case, it seems to me, of the kind of ‘active keyboard’ effect imagined by Merleau-Ponty—the organism selectively moves its body and receptors to try to discover the very stimuli that it predicts. In this way, different organisms and individuals may selectively sample in ways that both actively construct and continuously confirm the existence of different ‘worlds’. It is in this sense that, as Friston, Adams, and Montague (2012 , p. 22) comment, our implicit and explicit models might be said to ‘create their own data’.
Such a process repeats at several organizational scales. Thus, we humans do not merely sample some natural environment. We also structure that environment in (as we just saw) a wide variety of potent, interacting, and often cumulative ways. We do this by building material artefacts (from homes to highways), creating cultural practices and institutions, and trading in all manner of symbolic and notational props, aids, and scaffoldings. Some of our practices and institutions are also designed to train us to sample and exploit our human-built environment more effectively—examples would include sports practice, training in the use of specific tools and software, learning to speed-read, and many, many more. Finally, some of our technological infrastructure is now self-altering in ways that are designed to reduce the load on the predictive agent, learning from our past behaviours and searches so as to serve up the right options at the right time. In all these ways, and at all these interacting scales of space and time, we build and selectively sample the very worlds that—in iterated bouts of statistically sensitive interaction—install the generative models that we bring to bear upon them.
 The task of the generative model in all these settings is to capture the simplest approximations that will support the actions required to do the job—that (as we saw in chapter 8 ) means taking into account whatever work can be done by a creature’s morphology, physical actions, and socio-technological surroundings. PP thus harmonizes fully with work that stresses frugality, satisficing, and the ubiquity of simple but adequate solutions that make the most of brain, body, and world. Brain, body, and the partially self-constructed environment stand revealed as ‘mutually embedded systems’ (Varela et al., 2001, p. 423) working together in the service of situated success.

### Representations: Breaking Good?
There remains, however, at least one famously vexed issue upon which PP and enactivism (at least if history is any guide) seem doomed to disagree. That is the issue of ‘internal representation’. Thus Varela et al. are explicit that on the enactivist conception ‘cognition is no longer seen as problem solving on the basis of representations’ (p. 205). PP, however, deals extensively in internal models—rich, frugal, and all points in-between—whose role is to control action by predicting complex plays of sensory data. This, the enactivist might fear, is where our promising story about neural processing ‘breaks bad’. Why not simply ditch the talk of inner models and internal representations and stay on the true path of enactivist virtue?
This issue requires a lot more discussion than I shall (perhaps mercifully) attempt here.14 Nonetheless, the remaining distance between PP and enactivism may not be as great as that bald opposition suggests. We can begin by recalling that PP, although it trades heavily in talk of inner models and representations, invokes representations that are probabilistic and action-oriented through and through. These are representations that (see chapters 5–8 ) are fundamentally in the business of serving action within the context of rolling sensorimotor cycles. Such representations aim to engage the world, rather than to depict it in some action-neutral fashion. They remain, moreover, firmly rooted in the patterns of organism-environment interaction that served up the structured sensory stimulations reflected in the mature probabilistic generative model. The role of that generative model is to deliver an efficient, context-sensitive grip upon a world of multiple competing affordances for action.
 The shape of that grip is well captured by Itay Shani who writes that:

Actual sensory systems are not concerned with truth and accuracy as such but rather, with action and the need to maintain the functional stability of the organisms in which they are embedded. They do not report, or register, what is where like an idealized scientific observer but, rather, help organisms to cope with changing conditions in their external, and internal (somatic), environments. (Shani, 2006 , p. 90)
This is exactly the role played, if PP is correct, by the multilevel probabilistic generative models that guide perception and action.15
What are the contents of the states governed by these multilevel action-oriented probabilistic generative models? The generative model issues predictions that estimate various identifiable worldly states (including states of the body and the mental states of other agents).16 But it is also necessary, as we have repeatedly seen, to estimate the context-variable reliability (precision) of the neural estimations themselves. Some of these precision-weighted estimates drive action, and it is action that then samples the scene, delivering percepts that select more actions. Such looping complexities will make it hard (perhaps impossible) adequately to capture the contents or the cognitive roles of many key inner states and processes using the terms and vocabulary of ordinary daily speech. That vocabulary is ‘designed’ for communication (though it may also enable various forms of cognitive self-stimulation). The probabilistic generative model, by contrast, is designed to engage the world in rolling, uncertainty-modulated, cycles of perception and action. The representations thus constructed are ‘not actual re-presentations or duplicates of objects in the world but … incomplete, abstract code that makes predictions about the world and revises its predictions on the basis of interaction with the world’ (Lauwereyns, 2012 , p. 74). Within PP, high-level states (of the generative model) target large-scale, increasingly invariant patterns in space and time. Such states help us to keep track of specific individuals, properties, and events despite large moment-by-moment variations in the stream of sensory stimulation. Unpacked via cascades of descending prediction, such higher level states simultaneously inform both perception and action, locking them into continuous circular causal flows. Instead of simply describing ‘how the world is’, these models—even when considered at the ‘higher’ more abstract levels—are geared to engaging those aspects of the world that matter to us. They are delivering a grip on the patterns that matter for the interactions that matter .
 This suggests a recipe for peace in the disputes concerning internal representation. Varela et al. (1991) strongly reject appeals to ‘internal representation’. But for them, this notion implies the ‘action-neutral’ capture of what they call a ‘pregiven world’. Organism and world, they argued, are instead co-defined by a history of structural coupling: a kind of active ‘fitting’ of each to the other, rather than a passive ‘mirroring’. PP, I have tried to show, fully respects this intuition. It posits a hierarchical generative model that helps maintain the integrity and viability of a system by enabling it to minimize prediction errors and thus avoid compromising (possibly fatal) encounters with the environment. That distributed inner model is itself the result of self-organizing dynamics operating at multiple temporal scales, and it functions selectively to expose the agent to the patterns of stimulation that it predicts. The generative model thus functions—just as an enactivist might insist—to enable and maintain structural couplings that serve our needs and that keep us viable.
Could we perhaps have told our story in entirely non-representational terms, without invoking the concept of a hierarchical probabilistic generative model at all? One should always beware of sweeping assertions about what might, one day, be explanatorily possible! But as things stand, I simply do not see how this is to be achieved.17 For it is surely that very depiction that allows us to understand how it is that these looping dynamical regimes arise and enable such spectacular results. The regimes arise and succeed because the system self-organizes so as to capture patterns in the (partially self-created) input stream. These patterns specify bodily and worldly causes operating at varying scales of space and time. Subtract this guiding vision and what remains is just a picture of complex looping dynamics spanning brain, body, and world. Such a vision is surely correct, as far as it goes. But it does not explain (does not render intelligible) the emergence of a structured meaningful realm apt for perception, thought, imagination, and action.
Consider those same looping dynamics from the explanatory perspective afforded by PP, however, and many things fall naturally into place. With that schema in mind, we comprehend perception, imagination, and simulation-based reasoning as co-emergent from a single cognitive architecture; we see how that architecture simultaneously supports perception and action, locking them together in a circular causal embrace; we see why, and exactly how, perception and action are themselves co-constructed and co-determining; we see how, at longer timescales, statistically driven learning can unearth interacting distal and bodily causes in the first place, revealing a structured world of human-sized opportunities for action; and we understand  how it is that unexpected omissions and absences can be every bit as salient and perceptually striking as the most concrete of ordinary perceptibles. We appreciate all this, moreover, from a perspective that both accommodates and unifies impressive swathes of work in machine learning, in psychophysics, in cognitive and computational neuroscience and (increasingly) in computational neuropsychiatry. This is surely encouraging. Perhaps models in this broad ballpark offer our first glimpse of the shape of a fundamental and unified science of the embodied mind?

### Prediction in the Wild
Our neural economy exists to serve the needs of embodied action. It does so by initiating and sustaining complex circular causal flows in which actions and perceptions are co-determined and co-determining. These flows enact structural couplings that serve our needs while keeping the organism within its own specialized window of viability. All this is orchestrated, or so our story suggests, by a multilevel generative model tuned to predict task-salient aspects of the current sensory signal.
Is this an inner economy bloated with representations, detached from the world? Not at all. This is an inner economy geared for action that aims to lock embodied agents onto opportunities in their worlds. Dynamically speaking, the whole embodied, active system here self-organizes around the organismically-computable quantity ‘prediction error’. This is what delivers that multi-level, multi-area grip on the evolving sensory barrage—a grip that must span multiple spatial and temporal scales. Such a grip simultaneously determines perception and action, and it selects (enacts) the ongoing stream of sensory bombardment itself. The generative model that here issues sensory predictions is thus nothing but that multi-level, multi-area, multi-scale, body-and-action involving grip on the unfolding sensory stream. To achieve that grip is to know the structured and meaningful world that we encounter in experience and action.
That grip, in the somewhat special case of the human mind, is further enriched and transformed by layer upon layer of sociocultural structures and practices. Steeped in such practices, our predictive brains are empowered to redeploy their basic skills in new and transformative ways. Understanding the resulting interplay between culture, technology, action, and cascading neural prediction is surely one of the major tasks confronting twenty-first-century cognitive science.

### Summary structure

